import pandas as pd
import torch
from sentence_transformers import SentenceTransformer
import subprocess
import json
import time, re
import os

# === Ollama CLI æ¨™è¨»å‡½æ•¸ ===
def run_ollama_command(command, user_input):
    """é€é subprocess åŸ·è¡Œ ollama run ä¸¦å›å‚³ stdout"""
    full_command = f'echo "{user_input}" | {command}'
    start_time = time.time()
    process = subprocess.Popen(full_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdout, stderr = process.communicate()
    end_time = time.time()
    response_time = end_time - start_time
    return stdout.decode('utf-8').strip(), response_time

def download_model(model_name):
    """æª¢æŸ¥ä¸¦ä¸‹è¼‰æ¨¡å‹"""
    print(f"Checking if the model '{model_name}' is already downloaded...")
    download_command = f"ollama pull {model_name}"
    process = subprocess.Popen(download_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    stdout, stderr = process.communicate()
    if process.returncode == 0:
        print(f"Model '{model_name}' has been successfully downloaded.")
    else:
        print(f"Failed to download the model '{model_name}'. Error: {stderr.decode('utf-8')}")
        exit(1)

def extract_json_from_text(text, default_value=3):
    """
    å¾æ–‡å­—ä¸­æŠ“å–ç¬¬ä¸€å€‹ JSON ç‰©ä»¶ï¼Œè§£æç‚ºå­—å…¸
    text: æ¨¡å‹å›å‚³çš„æ–‡å­—
    default_value: è‹¥æŠ“ä¸åˆ°æˆ–è§£æå¤±æ•—ï¼Œå¡«å…¥é è¨­å€¼
    """
    try:
        # æŠ“å‡ºç¬¬ä¸€å€‹ { ... } çš„éƒ¨åˆ†
        match = re.findall(r'"(\w+_level)":\s*([1-5])', text)
        json_match = {k:int(v) for k,v in match}
        print(f"Extracted JSON: {json_match}")
        if len(json_match) == 4:
            return json_match
    except Exception as e:
        print(f"âš ï¸ JSONè§£æå¤±æ•—: {e} | åŸå§‹æ–‡å­—: {text[:100]}...")
    
    # å¤±æ•—å°±å›å‚³ Noneï¼Œè®“å‘¼å«ç«¯çŸ¥é“éœ€è¦é‡è©¦
    return None

def run_ollama_with_retry(command, prompt, max_retries=3, retry_delay=1):
    """
    å¸¶æœ‰é‡è©¦æ©Ÿåˆ¶çš„ Ollama åŸ·è¡Œå‡½æ•¸
    command: ollama å‘½ä»¤
    prompt: è¼¸å…¥æç¤º
    max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸
    retry_delay: é‡è©¦é–“éš”ï¼ˆç§’ï¼‰
    """
    for attempt in range(max_retries):
        response, rt = run_ollama_command(command, prompt)
        labels = extract_json_from_text(response)
        
        if labels is not None:
            return labels, rt, response
        else:
            print(f"âš ï¸ ç¬¬ {attempt + 1} æ¬¡å˜—è©¦å¤±æ•—ï¼Œ{f'{retry_delay}ç§’å¾Œé‡è©¦...' if attempt < max_retries - 1 else 'å·²é”æœ€å¤§é‡è©¦æ¬¡æ•¸'}")
            if attempt < max_retries - 1:
                time.sleep(retry_delay)
    
    # æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—ï¼Œå›å‚³é è¨­å€¼
    print("âŒ æ‰€æœ‰é‡è©¦å¤±æ•—ï¼Œä½¿ç”¨é è¨­å€¼")
    default_labels = {
        'stress_level': 3,
        'happiness_level': 3,
        'humor_level': 3,
        'encouragement_level': 3
    }
    return default_labels, 0, ""

def save_progress(df, vectors, checkpoint_dir="./data", prefix="chicken_soup"):
    """
    å®šæœŸå„²å­˜é€²åº¦åˆ° ./data ç›®éŒ„
    """
    # å»ºç«‹æª¢æŸ¥é»ç›®éŒ„
    if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir)
    
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    
    # å„²å­˜ CSV æª”æ¡ˆ
    csv_filename = f"{prefix}_checkpoint_{timestamp}.csv"
    csv_path = os.path.join(checkpoint_dir, csv_filename)
    df.to_csv(csv_path, index=False, encoding="utf-8-sig")
    
    # å„²å­˜å‘é‡æª”æ¡ˆ
    vector_filename = f"{prefix}_vectors_checkpoint_{timestamp}.pt"
    vector_path = os.path.join(checkpoint_dir, vector_filename)
    torch.save(vectors, vector_path)
    
    print(f"ğŸ’¾ æª¢æŸ¥é»å·²å„²å­˜åˆ° {checkpoint_dir}: {csv_filename}, {vector_filename}")

def load_latest_checkpoint(checkpoint_dir="./data", prefix="chicken_soup"):
    """
    å¾ ./data ç›®éŒ„è¼‰å…¥æœ€æ–°çš„æª¢æŸ¥é»
    """
    if not os.path.exists(checkpoint_dir):
        return None, None, 0
    
    # å°‹æ‰¾æœ€æ–°çš„ CSV æª¢æŸ¥é»æª”æ¡ˆ
    csv_files = [f for f in os.listdir(checkpoint_dir) if f.startswith(prefix) and f.endswith(".csv") and "checkpoint" in f]
    if not csv_files:
        return None, None, 0
    
    # æŒ‰æ™‚é–“æ’åºï¼Œå–å¾—æœ€æ–°çš„æª”æ¡ˆ
    csv_files.sort(reverse=True)
    latest_csv = os.path.join(checkpoint_dir, csv_files[0])
    
    # å°æ‡‰çš„å‘é‡æª”æ¡ˆ
    vector_file = csv_files[0].replace(".csv", ".pt").replace("_checkpoint_", "_vectors_checkpoint_")
    latest_vector = os.path.join(checkpoint_dir, vector_file)
    
    if os.path.exists(latest_vector):
        df = pd.read_csv(latest_csv)
        vectors = torch.load(latest_vector)
        
        # è¨ˆç®—å·²è™•ç†çš„æ•¸é‡ï¼ˆæ’é™¤ None å€¼ï¼‰
        processed_count = df[df['stress_level'].notna()].shape[0]
        
        print(f"ğŸ”„ å¾æª¢æŸ¥é»æ¢å¾©: {latest_csv} (å·²è™•ç† {processed_count} æ¢)")
        return df, vectors, processed_count
    
    return None, None, 0

# === ä¸»ç¨‹å¼ ===
if __name__ == "__main__":
    model_name = "gemma3:4b"
    save_interval = 100  # æ¯ 100 æ¢å„²å­˜ä¸€æ¬¡
    data_dir = "./data"  # æ‰€æœ‰æª”æ¡ˆéƒ½æ”¾åœ¨ ./data ç›®éŒ„

    # ä¸‹è¼‰æ¨¡å‹
    download_model(model_name)
    command = f"ollama run {model_name}"

    # === è®€å…¥é›æ¹¯è³‡æ–™ ===
    csv_path = os.path.join(data_dir, "chicken_soup.csv")
    df = pd.read_csv(csv_path)
    for col in ['stress_level', 'happiness_level', 'humor_level', 'encouragement_level', 'vector']:
        if col not in df.columns:
            df[col] = None

    # === åˆå§‹åŒ– SentenceTransformer æ¨¡å‹ ===
    st_model = SentenceTransformer('all-MiniLM-L6-v2')

    # === å˜—è©¦è¼‰å…¥æœ€æ–°çš„æª¢æŸ¥é» ===
    checkpoint_df, checkpoint_vectors, start_idx = load_latest_checkpoint(data_dir)
    if checkpoint_df is not None:
        df = checkpoint_df
        # æ›´æ–°å‘é‡è³‡æ–™
        for i, vector in enumerate(checkpoint_vectors):
            if i < len(df):
                df.at[i, 'vector'] = vector
        print(f"ğŸ”„ å¾ç¬¬ {start_idx} æ¢é–‹å§‹ç¹¼çºŒè™•ç†")
    else:
        start_idx = 0
        print("ğŸš€ é–‹å§‹æ–°çš„è™•ç†ä»»å‹™")

    # === å°æ¯æ¢é›æ¹¯æ¨™è¨»å¿ƒç†æŒ‡æ¨™ & è¨ˆç®—å‘é‡ ===
    total_items = len(df)
    
    for idx in range(start_idx, total_items):
        row = df.iloc[idx]
        text = row['text']

        # è·³éå·²è™•ç†çš„é …ç›®
        if pd.notna(df.at[idx, 'stress_level']):
            continue

        prompt = f"è«‹å°‡ä¸‹é¢é›æ¹¯æ¨™è¨»å¿ƒç†æŒ‡æ¨™ï¼ˆ1~5ï¼‰ï¼š- stress_level: å£“åŠ›æ„Ÿï¼ˆ1~5ï¼‰ - happiness_level: é–‹å¿ƒç¨‹åº¦ï¼ˆ1~5ï¼‰ - humor_level: å¹½é»˜ç¨‹åº¦ï¼ˆ1~5ï¼‰ - encouragement_level: é¼“å‹µéœ€æ±‚ï¼ˆ1~5ï¼‰ - é›æ¹¯å…§å®¹ï¼š{text} - è«‹åªå›å‚³ JSON æ ¼å¼ï¼Œä¾‹å¦‚ï¼š{{stress_level:?, happiness_level:?, humor_level:?, encouragement_level:?}}"

        # ä½¿ç”¨å¸¶æœ‰é‡è©¦æ©Ÿåˆ¶çš„å‡½æ•¸
        labels, rt, response = run_ollama_with_retry(command, prompt)
        
        df.at[idx, 'stress_level'] = labels['stress_level']
        df.at[idx, 'happiness_level'] = labels['happiness_level']
        df.at[idx, 'humor_level'] = labels['humor_level']
        df.at[idx, 'encouragement_level'] = labels['encouragement_level']

        # è¨ˆç®—æ–‡æœ¬å‘é‡
        df.at[idx, 'vector'] = st_model.encode(text, convert_to_tensor=True)

        # é€²åº¦é¡¯ç¤º
        if (idx + 1) % 20 == 0:
            print(f"âœ… å·²è™•ç† {idx + 1}/{total_items} æ¢é›æ¹¯ï¼Œè€—æ™‚ {rt:.2f}s")

        # å®šæœŸå„²å­˜
        if (idx + 1) % save_interval == 0:
            print(f"ğŸ’¾ é”åˆ° {idx + 1} æ¢ï¼Œé€²è¡Œå®šæœŸå„²å­˜...")
            vectors = df['vector'].tolist()
            save_progress(df, vectors, data_dir)
            print(f"âœ… ç¬¬ {idx + 1} æ¢å·²å„²å­˜å®Œæˆ")

    # === æœ€çµ‚å„²å­˜è³‡æ–™åº« ===
    print("ğŸ’¾ æ­£åœ¨é€²è¡Œæœ€çµ‚å„²å­˜...")
    
    # æœ€çµ‚å‘é‡æª”æ¡ˆè·¯å¾‘
    final_vector_path = os.path.join(data_dir, "chicken_soup_vectors.pt")
    torch.save(df['vector'].tolist(), final_vector_path)
    
    # æœ€çµ‚ CSV æª”æ¡ˆè·¯å¾‘
    final_csv_path = os.path.join(data_dir, "chicken_soup_with_features.csv")
    df.to_csv(final_csv_path, index=False, encoding="utf-8-sig")
    
    # æ¸…ç†æª¢æŸ¥é»æª”æ¡ˆï¼ˆå¯é¸ï¼‰
    print("ğŸ§¹ æ¸…ç†æª¢æŸ¥é»æª”æ¡ˆ...")
    if os.path.exists(data_dir):
        for file in os.listdir(data_dir):
            if file.startswith("chicken_soup") and "checkpoint" in file:
                file_path = os.path.join(data_dir, file)
                os.remove(file_path)
                print(f"ğŸ—‘ï¸ å·²åˆªé™¤æª¢æŸ¥é»æª”æ¡ˆ: {file}")
    
    print("âœ… é›æ¹¯è³‡æ–™åº«å»ºç«‹å®Œæˆï¼ŒåŒ…å«å¿ƒç†æŒ‡æ¨™èˆ‡å‘é‡")
    print(f"ğŸ“ æœ€çµ‚æª”æ¡ˆä½ç½®:")
    print(f"   - å‘é‡æª”æ¡ˆ: {final_vector_path}")
    print(f"   - ç‰¹å¾µæª”æ¡ˆ: {final_csv_path}")